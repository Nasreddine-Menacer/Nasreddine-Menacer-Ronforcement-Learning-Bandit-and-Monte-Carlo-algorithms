{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ronforcement Learning (Bandit and Monte Carlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd5ycVdn4/8+1vfea3U2y6YRU2ISEFiIBIWAARaQJKEVUxK7w04dH/T0+iu2xgCIgIAgCAiooHUNJSA/pddN3s73vzu7Mzsz5/nHfMzvbJ2W2ZK7367Wvnbnnnpkzmey57tOuI8YYlFJKha+I4S6AUkqp4aWBQCmlwpwGAqWUCnMaCJRSKsxpIFBKqTAXNdwFOFZZWVlm/Pjxw10MpZQaVTZs2FBrjMnu67FRFwjGjx/P+vXrh7sYSik1qojIof4eC1nXkIg8JiLVIrJtkPPmiYhHRK4OVVmUUkr1L5RjBE8Alwx0gohEAvcDb4SwHEoppQYQskBgjHkfqB/ktK8ALwLVoSqHUkqpgQ3brCERKQCuAh4arjIopZQa3umjvwa+a4zxDHaiiNwhIutFZH1NTc0QFE0ppcLHcM4aKgGeFRGALGCpiLiNMf/oeaIx5mHgYYCSkhLNkqeUUifRsAUCY0yx77aIPAH8q68goJRSKrRCOX30r8AqYKqIlInIrSJyp4jcGar3VEqpkabT4+W5dYfxeEduZ0bIWgTGmOuO4dxbQlUOpZQaTh/sreG7L25lQnYS88ZnDHdx+qS5hpRSKoSqmp0ANLd3DnNJ+qeBQCmlQqimxQoELR3uYS5J/zQQKKVUCNW22oHAqYFAKaVOWKfHy82PrWXdwcGSFowcXS2C4LqGjDG8sb2SRocrlMXqZtRlH1VKha+yhnbe21PDjIKUkA28er0GA0RGyHG/RlN7J3/6YD+Xzsz3B4LWQbqGWp1u7npmIwkxkby6tZLPn1PMfZ+YftxlOBYaCJRSw8rp9vDMmsPccNY4YqIG7qQ42thu/+4IWXnuf2MXGw428MIXzz7u1/j3lgp++59SfvufUv+xvsYImhydfPIPK7n7wsnUt7l4d3dX5oTNZY3H/f7HSgOBUuqEtTndVDZ3MDE76Zifu7K0lh++soP81HgumZE34Lm+QFBu/z4W7+6uJi0hhorGdqIiI7hoem6f52050sS+mtZ+X2dfTSsrS2u5aeH4fs/ZU9XS61hrH2ME2yua2FfTxlef3URGYgxzx6bxo2Uz+PtH5Ty1+iAOl5uEmNBX0zpGoJQKmsPlpqGtd9/1DY+u4cJfvoe3x6KpiqZ2Hnl/f6/jgart6ZVb+rkCbnW6ufOpDeyqbPa3BI42tvOPj8qpbAquZVDW4OCWx9dx7cOr+Pkbu/nhK9sxxirT2gP1vL2jyn9ueWM7rU63//FAHq/hwl++x33/3M7RxnZcbi+7Kpt7nbe7soXZRWnERXdVsb4xAq/XsPlII8YYDtc5AJg7No0IEe6+cDIzC1NZNDWbTo9h3cGGoD7fidJAoFSYand56PR4j+k5//PvnVz78OpexzcdsSrxuh5B4tm1R/jxqzvZUdFVWbo9Xj4srfXf9/Wh99cV8uSqg7y+vZI/f3jQ3yIoa2jna89t4qnVB4Mq98/f2A1AR6eXIw0OyhraKa22rvqv+eMqbntyPYfq2vB6DRVN7XR6DE63136Oh+pmK+D8/aNy/2uuO1jPoyv2c9lvV1DV3D0g7alqYWpuEkXpCf5jDY5OHl95gGUPruCKB1fy9s5qDtY5iI4UXrjzbNZ/fwmLp+YAMG98OilxUXzv71vZX9PKjY+u4dMPfcgLG8qC+rzHSgOBUiNQS0cnv31nL05338l5G9pcXPSr99hW3nTc73Hafa9z+5PHtu3rtvImdle1dOvm8F3VAr0qxN2VVhfJioCK/4UNZVz/6Bq2llllr2n1tQiaerUcPF7Ds2uPAODs9HK0qXuX0IHaNgC+88Jm3gq4qu/5Gu/s7NrypNNjvcc7u6q7XfXf//ouqluc/seb7Sv4X7yxmwt/+R51rU5W768jIzGGpNgof0vC4zX+WUxH6h18+ZmN1LW5mJKbTGF6vP/1Nxxq4Iev7OBgrfXvtfFwA4fr2yhKT+g1MJ0QE8VfbjuLpvZOrvnjalaU1tLq9NAWoimoGgiUGoGeXHWIX721h6dXH+7z8Y+ONLC3upWNh4+v68A3NTFwcHIwxhgO1FgVb2Af+Fs7uyrgnl01u+3zVpbWcrjOwb0vbeFfWyoA2HDIqjwDF1wdqGvr9vyXN5dzuN6qOPfVtHK0sZ2k2K4+8/01bTR3dPL8+jJuf3I97a7egXN/TSutTjeLp3bt2x4TFcF/dlVT29rVglmzv57yxq6g5hvcXVFaS4vTze/f3cfeqhZOy0/mzHHpvLG9ko/sltB6uwvn31sr+Lf9+abmJfOFRRMBq+vH582vn8/pY1LYWtbEoToHYzO7Wg2BZhWm8ZWPTaK21cmE7ET+/ZVzufns8X2ee6I0ECg1ApQ1OPgooFKPsNKzs+1o31f8uyutbo3qZicOl5trH17l754Jxrby3v3afZ/XxIZDVrlqW13+RVG+K/21B+q5//VdjEmNA6AyoEXgcLk5WNdGTGQE6w7W8/iHB/jr2iP+1oGvvNUtTrKSYoHu4wRuj5dfv72X0/JT+OyCceyraeNoYwdnjEv3n3Owro1DtV2V99Nruu/P3unx+t/nyrkF/uNXzhnDhkMN/kB63uQs6tpc7Dja9e/S0uGmqb2T3VUtxEVH8JfVh9hT1crknGQumJpNbasLYyArKZYnPjzIH9/b5w+U84szmF2UxoIJmRz86WWcPiYFgOhIITcljlmFqWwttwLBuIy+AwHATQvHc97kLL598VQiTmA662A0ECg1jLaWNbH4F+9y7v3Luer3H3LEvvr1XbHvrOg9+wS6rsirWzrYUtbE6v31rAzofunLT1/bxYq91jm+ABM4mNmX//rnNr73961AVzcMdAWCv649TFJsFC9/5VwixOoaKq1u4Z+bytlb1YoxsHRmHh2dXv97A6TGR/sr6JoWJwsmZJAQE8nmI12Bb1dlC4fqHNxxfjGTc5Nodbpp7/RwVnEGSbFRnJafQkenlzUH6gBr3v9r2yr9z/d4DcseWMm3X9hCXHQEH5tm9b8nxkRyTUkRHq/h8ZUHALj4dGu20vsBZWzp6GTj4QaMgVvOLsbp9tLe6WFybhI3LRzP/Z+aya3nFnP9/CIAfvLaLl7bVsG88ek8/4WFpMRF+18r2b49Ji2eyAhhRkEqTe2dtDrdjM1M7PffPy46kqduPYtLZ+YP+D2dKJ0+qtQwemN7JYfrHXzunPE8vvIguypbKMpIoNruLtlZ0UxDm4v0xJhuz/NVxNUtTv84QWC3jMdruO6R1UzPT+HepdOoa3Xx0Hv7qG7u4NzJWWy1n+PxGowx2BtEdeNye9le3kx0pFDd0sHy3VY/e3ZyLFvLm/B4DXuqWphRkEpWUizZybG8vq2S39lz57+2ZDIAny4p4h+bjrK3upUJWYksnpZDclwUv357L+WN7dS0OMlLiWNGQSqbyxppdbr5zdt7/BXk1NwU6gMGoa8+s5AbzhrL9qPN3PDoGt7bY3Vv3bxwPE98eIC6VicH69rYfrSZnfYgdXpCDMlx0eSmxJKVFMvcsemkJUSzer/VPXX2xEwA3t/T1VXW4Ojk9W0VREUIXzh/Ak98eICOTi9TcpOJjBA+M28sYAVtEeE37+ylucPNpJzeU2h93Vk5yVbL58yAVs288em9zh9qGgiUOkn6q1AHsqeqhfGZCXz9oik8vvIg+2pauYhcf785wLt7qrlqbqH/fpOjk9Karq4hX6VeERAIDtS2sfZAPWsP1JOdHEumHUgO2n3w2+3ndHoMze1uUhOsK9Z/birHawxXzS1kZ0UzLo8XlwdueWydf+bPp84o5KH39nHdI6sprW5l4QSrEs1LjWdzQPfUX1YfIj0hmoUTMklLiKbR0cmiqdn81+XT2VvVwkPv7eOy335Ae6eH7ORYZhem8udVh/jRK9t5fn2Zv7uoKCOerGSr/J8+s5DcFKsbyrdm4YO9tWQkxvCJ2fk8tvIAZ/7P2/4yFKTFc/eFkyjOss69pqSI1PhoIiOELy6ayE9e28W88emMzUggQsDp9jIxO5F9NW3c/9ouyhvb+dqSyaQnxrBwQibLd9cwuUdFn5YQw9cvmsJLH5VxpL69z7UUKXFR/nMBpuWl8PY3zqcgLYH4mMj+/nsMGQ0ESh2He17cgsdr+PmnZ/uP/fS1XWwtb+KZ2xd0O7e5o5PyhnZOy0/x369udjIpJ4k9VS1MH5NCSlw0Ocmx/imN1S0dXDQ9l+3lTbyyuYKlM/P543v7cbo9/OHdfXiNdZVZ3eL0zyyqCJhRszNguuaGQw3+LqBDdQ6cbg+H6x0UZyVyoLaN7/9zGwsmZHDFnALufWkrDpeHRkdnt5ksviBQMi6db108BRH4w7v7AJica1V8iXaF9tkF43hu3RFqW11cOiOPiAhhZkEqH+ytZUpusv2cZJ69YyFXPrgSgJyUWArS43F9cIDn11tTJGtbnWQkWlfyyXHRvP/txd1m4eSmxJKfGkdFUwdFGQnMLkxjfGYCMVERfOmCSWwrb+KcSVkstruEAL558VT/7S8smsiyOWOIioggOjIC34SlLyyayHde2EJ5YzvT8pL52pIpANx+/gSKs5L8lXlP88ZlcKS+nIl9tAh8/ftp8V3dRZNykvt8neGgYwRq1Gp0uLj3pS0nlJzL1zVyLNpdHv7+UTnv7uk+42b57mo22QuFAt33j2188vcf4nR7WHugnlk/eJMlv3qPo43tHKp3MNmuECZmJ/lXtFa3OMlNieXy2WN4f08Nb26v4ldv7eHB5fv42LQcfvLJmdxw1lhqW53sswcoA7uGdlY0ExUhXDYrny1lTazYW0tUhFDX5mL70Wa8pqtL4pXNR/ne37fxu//sxeHyMDYjgcdWHmDT4e6Dz19fMoVnbl9AVGQE19ndImBV6tA1UPypMws5vcAKegvtLpcZBanWuQGV5JyiNGbax9MSYjhvcjaXzcpnyWm5ZNtdKEUBFf/YzIRuA6Yiwo0LxgHQ6fYSESG8880LeONr53Pl3AK+f/n0bkGgL/mp8f73mpZnfY4r5xTga9gVBQzknj0xa8DcP+dOziIqQphuB/xATfZeBD27+EYKDQRq2Blz7JUxwJvbq/jr2iP86JUdx/W+Hq/hvPv/w6Kfv9vtCjrQ3X/9iFufWOcfxAVrKqTT7aWmxUmTw/oDb3O6Ka1uxeHy+GfWrD1Qz+/e2cu/tlTQ3ulhV0ULj6044H+d59YdwRhrmiHApJwkPjrcyCPv76fR0UlOchwfm5aD22t4das1JfFPN5fwx8+WcN38sRQGVFILJ2RS1+aio9NqHeysaGZSThJzi9KobXXS3OFm2ZwxALy7y+rr75m07Y/v7WdqbjK3nD2eI/XtvLmjinMnZfkfn5qX5M8FNDYzgYnZif5yA/z0k7O4aeE4ZhemMqcozV8ugKUz8jl7Yianj0nt9p6P3TKPW84ez1nFGaTGR/Pg9Wfw6M0lfMxeWFU0wIwagGvnWQO184utzxIZIcfcPefz19sXsOrejxETFUGSndahIC1+kGd1uWpuAe99Z7G/6yrQxdOtweirzyzs9dhIoF1Datg9tfoQD7+/nw++s/iY/ojb7Urv9e2V/MJrgp5e9/q2Ss4cl06b081R+yr6V2/t4ZGbSrqdV1rdwsubjwLgMYYnPjcfgHd2dc2bL61p4cxxGWwrb/J3LVQ2dfCfndV84/lNeA3+q8t1B+v5YG8NnzqjkFc2H+W5ddZCqSl214qvYv3xqzsBa2BxvD1guu5gPSJw/pRsf5eNb+AR4Io5Y1i1v47Lf7eC/71qJjsrWlgwIcNf8SbERHLjgnG8tLGc5fbaAV/lCfDLT8+mpaOTcyZl4bJXG7c63XxmXhGbjlgDuL4rf59rSop4Z1e1f3bM/OIM/2vetHA82cmx/iAxszC1V5cZWAPPP1h2eq/jvuA4dpBAkJkUy+p7LyQ9MXrA84IReLWeHBdFi9PdrStqMCLSb+CYlJPEwZ9edsJlDBVtEahht+lII2UN7TQ4jm0rP98qVofLE3SmxtpWJ3f+ZQMLfvKOP0fMGWPTeG9PTa988X/bUEZkhHDZzHzW7K/3p2PYfrSZ8fYiIF+f/taAFb6VTR388f39TM1L4cUvLuSpz59FVlIMj3ywnzaXh6Uz85hVmEplcwfjMxP8A5nL5hTwlY9N8r9OTkosOcmxxERFUNvqIic5lujIrj/ZdLuvOjMxhsL0rvL8z793UNncwczCNKbb89cXT8vx989vLW8iJznW/xywAskt5xQzOTeZaXkppMRFERMVweJpORSmxxMTGdFrvvsXFk3k+S8s7PPfuTgrkS9dMOm4r86DDQQAealxxEad3AHXJHtw91gCwWimgUAds+W7q/3TF0+G8gZrkLOiqf+Mks0dnTy16mC3LqTqgJk1fSUfa+ojsPjm33u8hv99dRcAX10yBZfb609DUNbgoLmjk39trmDRlGwun5VPe6eHH72yg//sqqKq2VrUFBcdwd6qVnYcbebJVYdIsAdLV+2vY2dFM586o4Azx2Vw7uQsJuUkUdXsJCUuirMnZlFid8vcc+lp/iv8jMQYvnnxVG5aaPV75yTHEREhFNpXmfmp3SulafnJjEmN49fXziEvtat1sKWsicgI4ROz8kmNj+aXn57NNy+aQlJsFBPsVkdCTGS3weCogABjTY0s4vr5Y0mKjWJKbjLTx6R0OyfUSsanc9u5xSzpJ0NoqPnm/RekDR6ITgUh6xoSkceAy4FqY8yMPh6/AfiufbcV+KIxZnOoyqNOnu++sIV5xRk8eP0ZJ+X1fPljKps6evUh+7y4oYwfvrKDecUZREUIk3KSqWru8M8a6ZnsrLyxnUU/W87/fnIm15QU+Y/vrbKu4LOTYzlc76AgLZ7zJmWRkRjDh/tquXJuAdc8tIri7ETKG9v5/LnF/u6Op1Yf4qnVh4iMEPJS4piQlcTmskbe2llFR6eHR24q4YZH1/DMGistxNKARUBXzS2gtLqNR28uIT4mks+fM54JWYl8/PTeFd1/f+J0ls7M9w+wFmYksL+2jTFp3fueU+Ki+fDeCwFrzv9F03OZXZjKL97cwwVTssmx+6o/FdAv/cB1Z7D0tx+waIqVbuHuCyf7WzeBvndZ16Doj6+agdtz7GM4JyI2KpLvXz40m7L0JdluERSESYsglGMETwAPAE/28/gBYJExpkFELgUeBs4KYXnUSWCModHRSVnDseeD74vHa6iwUwtXDJBSeLu99P/p1Yd5avUh3vnmImpanEzNS6aiqaPbgiOALUcacXsNv3l7L1fOKfAPcu6tbiE5Lorbzyvmf1/dxZTcJCIihKm5yeypaqW21cnRpg7/2MGZ49LJTIplTGqc/5jHa8hJjmXpzDx+8eYeAJ743DzOmZRFVlIMta0u5o5NY0xAf/E1JUVcU1Lk7yrJSYnjmnlF9CUyQlhgD7JC18yZni2CQDFRETxyUwluj5cj9e1cf9bYPs+bPiaF9d9f4l/g9I2LpvT7mj7JcSfe/z7aJMdFEx8dSXpCeHz2kLX1jDHvA/1uLGqM+dAY40uushoYmcPpqhun24vL46W8wTHoue/vqfEn4OpPTYsTtz3K6uveeW1rBcseWNGtq8gXCHyrSPdVt1LV3EFBWjzJcVHdAsGmI43+ee/lje28tLEMr9fwz03lfLivjim5yVw+awwiMM2e6jc1L5m9VS3sCejyio2K8E8FfPkr53JLQMKvnJQ4vnTBJG5eOI7PlBT5r7Dz7Jw7l/VICSBy/LNZfDNn8lN7z0bpKSoygvuvnsXsorR+z8lKiiUuevgXMY1kl8/K5/bzJxz3dzbajJRZQ7cCr/X3oIjcAdwBMHZs31c6amj45kPXtrp4e0cVhRnxTMvrPW8a4MHlpZQ1tPPx03PxGvrchjAw2+PRpnYcLjc/eGU7Vc1OvvT0Rl6482w8XkNptVVB+zJR7q9to8HRSW5KHJmJMeyubOETv1vBFxZN4K5nPgJgQnYiSbFRPLC8lK3lTTxtd9lcMSeVMWnxPHPbAv+g5OTcJNpcnm5rA2YVpvrLnJUU221+eE5yLBERwg+v6N7rmZcSz7by5pOaG8aX037MMUxlVCfm46fn8fHTB94t7VQy7IPFIrIYKxB8t79zjDEPG2NKjDEl2dnZ/Z2mQuDFDWXdtu3zBQKA255czyW//qDf51Y0dVDe2M53XtjCjY+u6fOccrtbKCUuisqmDp5fd4SqZifXnzWWjw438sHeGvZUtfhzxPv4MnXmJMeSnhjD6gN1bC1v4i+ru7JPnpafwteWTKasoZ2n1xxm6UzrD/ucidbc+IUTM8mwpwxOtWfU/GvzUVLjo7n7wsl8/pzibu9Z0G1Va99X55fPyueWs8cf0/zzwcwvzuD8KdmUjICcNOrUNKwtAhGZBTwKXGqMqRvOsqjePF7Dt1/YzE0Lx/vnegcGgoEYY/xdPS9vPoqIlVa458wT34yhM8elc6jOwYrSWoqzEvnBJ07njW2VPLPmsL+/3Nf/DrDhkDVd1Nci8E0mCtza77S8ZBZPzeHB689gXGYCMwpScbjcxPfRLeKbI3+0qYN549P77DsPrNyzA+bwB7pybkG3dMcnQ3ZyLE9+fv5JfU2lAg1bi0BExgIvAZ81xuwZrnKo/jU4XHiNlffGp68pmT3trGhmRWmtf2GS22vo9BiO2JW+12s4VNdGWYODv204wsTsRCZmJ3G0qZ21B+o5qziDmKgIri4p5J1d1Tz8/n7mFKV1WwBVa+9qNTYzwX9VD1bwihBr/vl5k7MRsdIs+GbgJMRE9dnvmxof7U+5MLuw7/71fHvWTkpclPaxq1NKKKeP/hW4AMgSkTLgv4FoAGPMQ8B9QCbwe/sP022MKen71dRQennzUSbnJPk3R6lq7pqv37NFEBkheLzGPyfd7fFy25/X+yvqQKXVrRSmx3PHk+v9q1vBmnETGSE8usJK83vWBKvCv/P8ifxrcwXlje3cu3SaP1d9bFQETreXoox4JmQlkpHY/er87IlZ/OW2Y5+A9vwXFlLW0N7v1X5sVCQ5ybGkxIfHTBIVPkIWCIwx1w3y+G3AbaF6f9W/dpen39S3jQ4Xd//VGmy9a7G1yjVwH1pfIBifmcDhegcer6Gu1emfs/72zmrKG7tm+0QIGMAYKxBsOtLA8t013H3hZP/q1Qum5mCM4aziDNYcqGd+sdUVlJ4Yw+Ofm8c/N5Vz6Yx8f2rm08eksPFwI/PGZyAi/hTLPuOzjm8RkIgMmttmSm6ytgbUKWekzBpSQ+RoYzsX/Pxdnrx1PgsmZNLp8XLPi1u5+exxzCpM42DARuTPrrNm2VS3ODHGWPPr7Ur+ta+ez/Ld1Xzp6Y1UNVuB4L09Nfzg5e1ECP68O2dPzKLF6aaisZ3NRxpZUVrL5bPye/XBiwg/u3oWK0vruvXFT8lN5tsfnwbA9PwUYqIiWDAhk42HGzlvsjXo6+sa8uWRHz/Ajk8n6rfXzSWEOwYqNSyGfdaQOnmcbg9vbq/E6zW0Od386q09vTbz3n7U2mzEl2phf00bL24sY9kDK3G43ByyNy6JihD/wKzL7aXR0cknf7+SR1ccIDk2iviYSH+F/fr2Cj7YW8Ndz2wkMTaSh248E7D2Z33os2fy5OfnMzE7ide3V9LqdHPbeRP6LP+4zMR+F0IBnD0pi833Xcw3LprCn24u4co51qCsLxAsm11AdKQwd2zoZtdkJMb0m49eqdFKWwSnkJc2lnPvS1v5/mWnER8TyW/f2Uunx8t3L5nmP+egve9srd3N4tuxCuBv68todHQiYk2t/CBg/9adFc3+sQJfH7lv8dSDy/fx4PJ9iMDf7lzItLwU8lLiiI4S/wrWa+cX0dTeyWn5Kf4UxcfD16V14WldqRlmF6Vx/pRsrp1fxBcWTdCuG6WOkQaCU4hvde2Dy0v54gUTAWt1b2AgOGBX/DX2YO5huysoISaSjYcb/Hl0JmYndQsEq/Z3ze71VcaBffMzC1I5Z1KWf3HZJ2bn4whojVwxp4Ar5pzcaZU+GYkxOr1SqROggWCUqW7pYNW+Oq6YU0Cjw8VbO6q4+sxCRMQ/b7/B0clbO6yc+Tsrmqlvc/m7T3wtgpoWK2gcrGsjNT6aBRMy2HykkcykWMZlJvjT7ybEROJwefhwX1cgqLLfJ3BNwItfPLvbyuHApGVKqZFNxwhGGLfH26tfP9BfVh/mq89uYl9NK39bX8a3X9jCljLf5uXtxNqVsW+qpddYO2WBtcjL3zXkaxHUOxifmcDsImugeEtZI+MyEv2BwJeCYcOhBv8GK74duABuPbeYb398ap/pI5RSo4P+9Y4wv357L1f9fmW/j/vSPby3u4a9dv6dP394kO//YyuH6hz2lEpwebxMz08hQqxWwYHaNk6773V/Bs2aFif7a1rZV93K2MxE5tiLqDo9hsm5Sf5NSwrTu1oHU+y9dX17uwL81+XT+fLirs1UlFKjj3YNjTA7KprZW92Kt5+tF/fbG5UH7qj10kfl/sfnFWdwoDae8sZ2irMScbo97KhoJiEmko5Oa6Vvfmoc5Y3tfOyX7wFwRXo8MwtTSU+IZk5RGjcuGOff+zYzMYavLZnCt/62mVanm+XfuiBsUvMqFS40EIwAz649zL6aVr532XSONrbj8RoaHC4yk2KpbXWyv6aN+cUZeL2GA7WtRAis3l+H1xiiIsSfxhlgTGocYzMSKG9sJzfF2uFq46EG2l0eJmYn8u2PT2NvVQu/fKsrq8dZxRkkx0Wz9ntL/FshxkZFcPmsfBZNzWbR5GxW7avjoum5FGeFbo6+Ump4aNfQECqtbuH59UfYH5DNE+DVbZW8uNG6qvdtzuKb1fOnFQe44dHVON0ejja109Hp5Yo5BTjdXjo9hi9dMJEffGK6v7smPzXev89rXqqVOrm8sZ0VpbUsnprDJTPyumXRLP3xpVwwNQeg2364IsID15/B4qhthokAABxFSURBVKk5REQIv7xmNpfMCJ+0vEqFEw0EQ+j/e2kb33lhiz9fvk+VvcNWo8PVle/fntVT2dRBp8dwsNbh7xYK3HpxXnEGt5xT7M/QmZ8Wx1h768HclDhmFnRt/eiryH25dLKSYoZ0H1ql1MikXUNDyLc3746KZmpanKzaX0ddq5MqO7vnpiON/nNrWrsGdcHK07O70tp1a2JOIpecnsfr2yuZbA/gLjktl2fXHWZSTpJ/PUF+ajwl49J55KYSJtgZPqFr68GzArZDVEqFLw0EQ8QYQ22rkwUTMli9v56VpbU8umI/5Q3tNNqpnTceDggEdgDw/X5y1UHWHaxnyWk5ZCfF8utr57CtvMm/uvfcyVls+8HHiYqM4KLpufzPlTM4c1w6ERHCRdO7b5A+uzCV/7lyxknPm6+UGp00EAyRVqebjk4vF0zNYVdlC8t3V7OnstWfsx+6dt0C/Hl+fPP91xyopyAtnt9cOxcRIS46kpLxGd3eI8o/0BvJjQvG9VsWERnwcaVUeNFAMER8V/Z5KXGcNzmbf2+p6DbbB2CT3SLISY6lpsVJp8dLvaNrU/ZPlxSSGKtfmVLq5NJa5SR7du1hEmOj+MTsMd2O+wJBdnIsS2fk8crmo72e2+J0k5UUQ0F6PH//qJy6Npd/C0aAT51RGNKyK6XCkwaCk+yel7YCVpfO5wI2P/dNB81KiuWMsekk2Inb2js9GAOJMZG0uTycPyXbPzvo/T3WLl4/+eRMxmUmDLppilJKHQ+dO3gSuD1ejDG0BuTgeX1bJesP1vvz/ge2COJjIrlu/lgum5lPQVo8sVERtNn5ha4+s5CspO5bJU7JTebsiVlD9GmUUuFGWwQnyOs1LPnVe1wzr4iLp1vz9BNjItld1cIXn97IuIwEclJieXVrJVERQpqdy/+/Lreyc372T2uIEGFmYSr/3lLBguJMpuQmc6R+Ilf9/kPAGjNQSqlQ0UBwgnZWNltZO480+RdvLZqazatbKwFrjwCv3dHvNb3zB33n49NocLg4e2Imv7h6NhERQlZSbLdWQc8WglJKnUzaNXSCVtl5+ssaHf70EBdMyfE/7vEa/4Bvj0lCAMwsTOX8KdlERUb02lD+poXWFM/+NppXSqmTIWQtAhF5DLgcqDbGzOjjcQF+AywFHMAtxpiNoSpPqKwstXbxKmto928Mc/6UbAD/No2ZSTGMzUjwrwIO1g+Xnc4Pl51+EkurlFK9hbJr6AngAeDJfh6/FJhs/5wF/MH+PWp0erysPVBPTGQEjY5O9la3kpUUQ15qHDnJsUzNS2bZ7DGkJcT0Wt0bDJHeaaiVUupkC1kgMMa8LyLjBzjlCuBJY4wBVotImojkG2MqQlWmk21LWSNtLg+Xzcrn31sq2HCw3p/y4XfXzSUzKYZJx9gKUEqpoTacYwQFwJGA+2X2sV5E5A4RWS8i62tqaoakcMH4sLQOEWvKJ8DRpg7yUqwUz2dNyNQgoJQaFYYzEPTV79HHcCoYYx42xpQYY0qys7NDXKzgrdxXy/T8lG6pnqfmJQ1jiZRS6tgNZyAoA4oC7hcCvfMujFAut5eNhxtZOCGTzMQY//E7F00cxlIppdSxG851BC8Dd4nIs1iDxE2jaXxgd2ULLreXuWPTEREeuamE4qxEf65/pZQaLUI5ffSvwAVAloiUAf8NRAMYYx4CXsWaOlqKNX30c6EqSyhsLrMyhc4qtLqFjmdWkFJKjQShnDV03SCPG+DLoXr/k2n70SYm5yQTE9XVk7alrJGMxBgKA/b/VUqp0UhXFg+ivs3FsgdW8tx6a4KT12s4Uu9gw6EGZhak6lx/pdSop7mGBlHR1I7Ha9hxtJnPPb6WNQfqcdiZQj+p+wMopU4BGggGUW2nj/7Priqqmp1cPD2XC6bmMDUviTlF6cNcOqWUOnEaCAbh20egqtn6/c2LpzI1TxeKKaVOHYOOEYjIJ0Vkr4g0iUiziLSISPNQFG4k8AUCgNioCCZmJw5jaZRS6uQLpkXwM+ATxpidoS7MSBQYCKblJRMVqePrSqlTSzC1WlW4BgGw9hqOtDeTmT4mdZCzlVJq9AmmRbBeRJ4D/gH4L4+NMS+FrFQjSE2zkxljUmjucLN46sjJc6SUUidLMIEgBWvl78UBxwwQHoGg1cnpY1J44PozhrsoSikVEoMGAmPMqEr9cLLVtDjJ1s3jlVKnsGBmDU0RkXdEZJt9f5aIfD/0RRt+6w/W0+p0ayBQSp3SghksfgS4F+gEMMZsAa4NZaFGgiP1Dj7z8GpioyKYqwvHlFKnsGDGCBKMMWt75NRxh6g8I8baA/V4vIZX7jqX6WNShrs4SikVMsG0CGpFZCL27mEicjUwavYNOF4bDzeQFBulq4iVUqe8YFoEXwYeBqaJSDlwALghpKUaATYebmROUZp/DYFSSp2qggkEh4wxS0QkEYgwxrSEulDDrdXpZndlM3ctnjTcRVFKqZALpmuoVER+DowNhyAA8O7uarwGFkzMHO6iKKVUyAUTCGYBe4A/ichqEblDRE7p0dNXNh8lOzmWs4o1ECilTn2DBgJjTIsx5hFjzNnAd7D2Hq4QkT+LyCnXd9LqdLN8dw2XzczX8QGlVFgIZkFZpIgsE5G/A78BfglMAF7B2oD+lLK1rAmX28sFmldIKRUmghks3gssB35ujPkw4PgLInJ+aIo1fPZUWcMgp+Wf0r1fSinlF9QYgTHm1h5BAABjzN0DPVFELhGR3SJSKiL39PH4WBFZLiIficgWEVl6DGUPid1VLaTGR5OjaSWUUmEimECQJiJ/F5EaEakSkRdFZNBd20UkEngQuBSYDlwnItN7nPZ94HljzFystBW/P8byn3S7K1uYmptMj5XUSil1ygomEDwOvAzkAwVYYwOPB/G8+UCpMWa/McYFPAtc0eMcg5XmGiAVOBpMoUPFGMOeyhZdTayUCivBBIJsY8zjxhi3/fMEEMxIagFwJOB+mX0s0A+AG0WkDGvg+St9vZA9ZXW9iKyvqakJ4q2PT0VTBy1ON1M0ECilwkiwuYZutGcPRYrIjUBdEM/rq2/F9Lh/HfCEMaYQWAo8JSK9ymSMedgYU2KMKcnOPvmzeYwxvL+nhp0VzQBMzdVAoJQKH8HMGvo88ADwf1gV+Yf2scGUAUUB9wvp3fVzK3AJgDFmlYjEAVlAdRCvf9Ks2lfHTY+tJT46EtBAoJQKL8HsUHYYWHYcr70OmCwixUA51mDw9T3OOQxcCDwhIqcBcUDo+n764XR7AWjv9JCXEkdqQvRQF0EppYZNv4FARH470BMHmzpqjHGLyF3AG0Ak8JgxZruI/AhYb4x5Gfgm8IiIfB2rtXGLMaZn91HIdXR6/Ld1fEApFW4GahHcCWwDnsfq0jnm+ZTGmFfpsfrYGHNfwO0dwDnH+ronW4uza5+daRoIlFJhZqBAkA98GvgM1o5kzwEvGmMahqJgQ6ktIBBM0fEBpVSY6XfWkDGmzhjzkDFmMXALkAZsF5HPDlXhhkprhxUIvn/ZaSydmTfMpVFKqaE16GCxiJyBNc3zIuA1YEOoCzXUWl1uYqIiuO28CcNdFKWUGnIDDRb/ELgc2Im1KvheY8wpuWl9a4eb5NhgZtIqpdSpZ6Da77+A/cBs++d/7fw7AhhjzKzQF29otDndJGogUEqFqYFqv+IhK8Uwa3W6SdJAoJQKU/3WfsaYQ0NZkOHU0uEmKU4DgVIqPAWTa+iU1+bSFoFSKnxpIMAaLNZAoJQKVxoIgFanRweLlVJha6Dpo1vpnTba71SaNdTq7CRZxwiUUmFqoNrvcvv3l+3fT9m/bwAcISvREHN7vHR0erVrSCkVtgadNSQi5xhjAhPD3SMiK4Efhbpwodbc0cm3nt8MoF1DSqmwFcwYQaKInOu7IyJnA4mhK9LQWbG3ljd3VAHoymKlVNgKdoeyx0UkFWvMoIngdigb8Vz2hjQAsdE6bq6UCk8DBgJ7/+BJxpjZIpICiDGmaWiKFnr1bS7/bR0sVkqFqwEvg40xXuAu+3bzqRQEABodLiIE/vnlc1g8NWe4i6OUUsMimP6Qt0TkWyJSJCIZvp+Ql2wINDg6SY2PZnZRGnZCPaWUCjvBjhFA1zRSsMYKRn3y/nqHi/TEmOEuhlJKDatBA4Ex5pTNQtrocJGeoIFAKRXegpoqIyIzROQaEbnJ9xPk8y4Rkd0iUioi9/RzzjUiskNEtovIM8dS+BPV0NZJekL0UL6lUkqNOMFsVfnfwAXAdOBV4FJgBfDkIM+LBB7E2uKyDFgnIi8bY3YEnDMZuBc4xxjTICJDOmLb6HAxfUzKUL6lUkqNOMG0CK4GLgQqjTGfw9qtLDaI580HSo0x+40xLqztLq/occ7twIPGmAYAY0x10CU/CeodLjJ0jEApFeaCCQTt9jRSt72WoJrgBooLgCMB98vsY4GmAFNEZKWIrBaRS4Ip9MnQ0emho9NLmnYNKaXCXDCzhtaLSBrwCLABaAXWBvG8vuZj9sxmGgVMxup6KgQ+EJEZxpjGbi8kcgdwB8DYsWODeOvBNTisxWQ6WKyUCnfBzBr6kn3zIRF5HUgxxmwJ4rXLgKKA+4XA0T7OWW2M6QQOiMhurMCwrkcZHgYeBigpKek3Nfax8K0q1sFipVS4G7RrSESeFJHbRWSaMeZgkEEArMp8sogUi0gMcC3wco9z/gEstt8nC6uraH/wxT9+jY5OQFsESikVzBjBE0A+8DsR2SciL4rIVwd7kjHGjZWe4g1gJ/C8MWa7iPxIRJbZp70B1InIDmA58G1jTN3xfJBj5e8a0sFipVSYC6Zr6D8i8h4wD+vq/U7gdOA3QTz3Vawpp4HH7gu4bYBv2D9DqsFuEehgsVIq3AWzjuAdrP0HVgEfAPOGeppnKDTYYwRp8doiUEqFt2C6hrYALmAGMAuYISLxIS3VEGhwuEiOjSImSvchUEqFt2C6hr4OICJJwOeAx4E8gltUNmI1OjpJS9RuIaWUCqZr6C7gPOBM4BDwGFYX0ajWoAnnlFIKCG5BWTzwK2CDPRPolNDQ5iJNA4FSSg0+RmCM+TkQDXwWQESyRWTUp6ZucHSSoTOGlFIqqAVl/w18FytLKFhB4S+hLNRQaHBoi0AppSC4WUNXAcuANgBjzFEgOZSFCrVOj5eWDreOESilFMEFApe98MsAiEhiaIsUer70Ehk6a0gppYIKBM+LyB+BNBG5HXgbeDS0xQqtRju9hHYNKaVUcOsIfiEiFwHNwFTgPmPMWyEvWQg1aMI5pZTyC2b6KHbF/xZYW1CKyA3GmKdDWrIQ8qWg1jxDSik1QNeQiKSIyL0i8oCIXCyWu7DSRF8zdEU8+XxdQ7pNpVJKDdwieApowEo2dxvwbSAGuMIYs2kIyhYy2jWklFJdBgoEE4wxMwFE5FGgFhhrjGkZkpKFUKPDRWxUBPExkcNdFKWUGnYDzRrq9N0wxniAA6dCEABrjEBbA0opZRmoRTBbRJrt2wLE2/cFa0+ZlJCXLkQaHJ26M5lSStn6DQTGmFO236TR4dJN65VSyhaWu7LUawpqpZTyC8tA0Ojo1DUESillC7tA4PUaGh0uXUOglFK2kAYCEblERHaLSKmI3DPAeVeLiBGRklCWB6Clw43XaJ4hpZTyCVkgEJFI4EHgUmA6cJ2ITO/jvGTgbmBNqMoSqN5eVayDxUopZQlli2A+UGqM2W+McQHPAlf0cd7/D/wM6AhhWfwa/IFAWwRKKQWhDQQFwJGA+2X2MT8RmQsUGWP+NdALicgdIrJeRNbX1NScUKGa7PQSOlislFKWUAYC6eOY8T8oEgH8H/DNwV7IGPOwMabEGFOSnZ19QoVq7rACQUq8BgKllILQBoIyoCjgfiFwNOB+MjADeFdEDgILgJdDPWDc3OG23jwuqAzcSil1ygtlIFgHTBaRYhGJAa4FXvY9aIxpMsZkGWPGG2PGA6uBZcaY9SEsEy2+FkGctgiUUgpCGAiMMW7gLuANYCfwvDFmu4j8SESWhep9B9PS4SYqQoiNCrslFEop1aeQ9o8YY14FXu1x7L5+zr0glGXxae1wkxwXhUhfQxhKKRV+wu6yuKWjk2TtFlJKKb8wDARuHShWSqkAGgiUUirMhV8gcLpJitWuIaWU8gm/QNDRSYq2CJRSyi8MA4F2DSmlVKCwCgTGGFqdbp01pJRSAcIqEDhcHjxeoy0CpZQKEFaBoNXpyzOkLQKllPIJq0DgyzOUpC0CpZTyC6tAoJlHlVKqt7AKBC12INDpo0op1SWsAoHDHiNIiNFAoJRSPmEVCFweL4CmoFZKqQBhVSM6O61AEKOBQCml/MKqRnT6WwSRw1wSpZQaOcIqELjc2iJQSqmewqpGdLo9gI4RKKVUoLCqEf0tgsiw+thKKTWgsKoRXW4v0ZFCRITuV6yUUj5hFQicbq+2BpRSqoeQ1ooicomI7BaRUhG5p4/HvyEiO0Rki4i8IyLjQlkel9urA8VKKdVDyGpFEYkEHgQuBaYD14nI9B6nfQSUGGNmAS8APwtVecAKBDp1VCmlugvl5fF8oNQYs98Y4wKeBa4IPMEYs9wY47DvrgYKQ1geXB5tESilVE+hrBULgCMB98vsY/25FXitrwdE5A4RWS8i62tqao67QE63RwOBUkr1EMpasa+pOabPE0VuBEqAn/f1uDHmYWNMiTGmJDs7+7gL5NLBYqWU6iWUaTjLgKKA+4XA0Z4nicgS4HvAImOMM4Tlwen2EhutgUAppQKFslZcB0wWkWIRiQGuBV4OPEFE5gJ/BJYZY6pDWBZAp48qpVRfQlYrGmPcwF3AG8BO4HljzHYR+ZGILLNP+zmQBPxNRDaJyMv9vNxJodNHlVKqt5Du0GKMeRV4tcex+wJuLwnl+/ek00eVUqq3sLo8dnm8mnBOKaV6CKtaUaePKqVUb2FVK+r0UaWU6i2sakWXTh9VSqlewqpW1OmjSinVW1jVijp9VCmlegubWtHjNbi9RgOBUkr1EDa1om+bSl1HoJRS3YVdINAWgVJKdRc2taLT4wE0ECilVE9hUyt2dQ2FzUdWSqmghE2t6NRAoJRSfQqbWtE/RqDrCJRSqpuwqRV1sFgppfoWNrWiU6ePKqVUn8ImEGiLQCml+hY2taJLp48qpVSfwqZWdHbqYLFSSvUlbGrFnJRYls7MIz0xeriLopRSI0pI9yweSc4cl8GZ4zKGuxhKKTXihE2LQCmlVN9CGghE5BIR2S0ipSJyTx+Px4rIc/bja0RkfCjLo5RSqreQBQIRiQQeBC4FpgPXicj0HqfdCjQYYyYB/wfcH6ryKKWU6lsoWwTzgVJjzH5jjAt4FriixzlXAH+2b78AXCgiEsIyKaWU6iGUgaAAOBJwv8w+1uc5xhg30ARk9nwhEblDRNaLyPqampoQFVcppcJTKANBX1f25jjOwRjzsDGmxBhTkp2dfVIKp5RSyhLKQFAGFAXcLwSO9neOiEQBqUB9CMuklFKqh1AGgnXAZBEpFpEY4Frg5R7nvAzcbN++GviPMaZXi0AppVToSCjrXRFZCvwaiAQeM8b8WER+BKw3xrwsInHAU8BcrJbAtcaY/YO8Zg1w6DiLlAXUHudzRxr9LCOTfpaRST8LjDPG9Nm3HtJAMNKIyHpjTMlwl+Nk0M8yMulnGZn0swxMVxYrpVSY00CglFJhLtwCwcPDXYCTSD/LyKSfZWTSzzKAsBojUEop1Vu4tQiUUkr1oIFAKaXCXNgEgsFSYo90InJQRLaKyCYRWW8fyxCRt0Rkr/07fbjL2RcReUxEqkVkW8CxPssult/a39MWETlj+EreWz+f5QciUm5/N5vs9TO+x+61P8tuEfn48JS6NxEpEpHlIrJTRLaLyFft46Puexngs4zG7yVORNaKyGb7s/zQPl5sp+rfa6fuj7GPn5xU/saYU/4Ha0HbPmACEANsBqYPd7mO8TMcBLJ6HPsZcI99+x7g/uEuZz9lPx84A9g2WNmBpcBrWHmoFgBrhrv8QXyWHwDf6uPc6fb/tVig2P4/GDncn8EuWz5whn07Gdhjl3fUfS8DfJbR+L0IkGTfjgbW2P/ez2MtuAV4CPiifftLwEP27WuB547nfcOlRRBMSuzRKDCN95+BK4exLP0yxrxP7xxS/ZX9CuBJY1kNpIlI/tCUdHD9fJb+XAE8a4xxGmMOAKVY/xeHnTGmwhiz0b7dAuzEygY86r6XAT5Lf0by92KMMa323Wj7xwAfw0rVD72/lxNO5R8ugSCYlNgjnQHeFJENInKHfSzXGFMB1h8DkDNspTt2/ZV9tH5Xd9ldJo8FdNGNis9idyfMxbr6HNXfS4/PAqPwexGRSBHZBFQDb2G1WBqNlaofupc3qFT+gwmXQBBUuusR7hxjzBlYO759WUTOH+4Chcho/K7+AEwE5gAVwC/t4yP+s4hIEvAi8DVjTPNAp/ZxbKR/llH5vRhjPMaYOVgZm+cDp/V1mv37pHyWcAkEwaTEHtGMMUft39XA37H+g1T5muf27+rhK+Ex66/so+67MsZU2X+8XuARuroZRvRnEZForIrzaWPMS/bhUfm99PVZRuv34mOMaQTexRojSBMrVT90L+9JSeUfLoEgmJTYI5aIJIpIsu82cDGwje5pvG8G/jk8JTwu/ZX9ZeAme5bKAqDJ11UxUvXoK78K67sB67Nca8/sKAYmA2uHunx9sfuR/wTsNMb8KuChUfe99PdZRun3ki0iafbteGAJ1pjHcqxU/dD7eznxVP7DPUo+VD9Ysx72YPW3fW+4y3OMZZ+ANcthM7DdV36svsB3gL3274zhLms/5f8rVtO8E+sK5tb+yo7V1H3Q/p62AiXDXf4gPstTdlm32H+Y+QHnf8/+LLuBS4e7/AHlOherC2ELsMn+WToav5cBPsto/F5mAR/ZZd4G3Gcfn4AVrEqBvwGx9vE4+36p/fiE43lfTTGhlFJhLly6hpRSSvVDA4FSSoU5DQRKKRXmNBAopVSY00CglFJhTgOBUgFExBOQrXKT2JlqReRrIpIw3OVTKhR0+qhSAUSk1RiT1Mfxg1hz52uHvlRKhZa2CJQahIjcDYwBlovIcvvYH0RkfWDOePv4QRG5384pv1ZEJtnHs0XkRRFZZ/+cYx9fFND6+Mi3glypoaQtAqUCiIgHazWqz0+MMc/1bBGISIYxpl5EIrFW4N5tjNlin/eIMebHInITcI0x5nIReQb4vTFmhYiMBd4wxpwmIq8APzXGrLSTpnWYriyTSg2JqMFPUSqstBsr8+NgrrHTgUdhbYwyHSstAFhpKHy//8++vQSYHpAqPsW++l8J/EpEngZeMsaUnYTPoNQx0UCg1DGyE5V9C5hnjGkQkSewcr74mD5uRwALjTHtPV7upyLyb6zcOKtFZIkxZleIiq5Un3SMQKngtGBtgwiQArQBTSKSi7VHRKDPBPxeZd9+E7jLd4KIzLF/TzTGbDXG3A+sB6aFpvhK9U9bBEp1F2/vDuXzujHmHuBh4DURqTDGLBaRj7Aywe7H6t4JFCsia7AutK6zj90NPCgiW7D+7t4H7gS+JiKLAQ+wA2tfYKWGlA4WK3US6TRTNRpp15BSSoU5bREopVSY0xaBUkqFOQ0ESikV5jQQKKVUmNNAoJRSYU4DgVJKhbn/B1Ycs+WlE5oMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.qstar = np.random.normal(size=k)\n",
    "    \n",
    "    def action(self, a):\n",
    "        return np.random.normal(loc=self.qstar[a])\n",
    "\n",
    "## k le nombre de bandits\n",
    "## nimsteps le nombre d'action de selection \n",
    "## Apossible la liste des actions possible au moment d'une action \n",
    "## A les actions selectionnée au moment d'une action \n",
    "# N[a,t]: le nombre d'action a selectionné au moment de l'action 0  \n",
    "# R[t]: reward au moment d'une action\n",
    "# Q[a,t]: la valeur estimé de a au moment d'une action\n",
    "        \n",
    "def action_greedy(k, netape, epsilon):\n",
    "    Apossible = {}\n",
    "    A = np.zeros((netape,))\n",
    "    N = np.zeros((k,netape+1))\n",
    "    R = np.zeros((netape,))\n",
    "    Q = np.zeros((k,(netape+1)))\n",
    "    bandit = Bandit(k)   # Initialisation du  bandit\n",
    "    \n",
    "    for t in range(netape):\n",
    "        if np.random.rand() < epsilon:\n",
    "            # toutes les possibilités des actions sont égales\n",
    "            Apossible[t] = np.arange(k)\n",
    "        else:\n",
    "            # Sélectionner les actions greedy comme actions possibles\n",
    "            Apossible[t] = np.argwhere(Q[:,t] == np.amax(Q[:,t])).flatten()\n",
    "\n",
    "        # Sélectionner une action au hasard parmi les actions possibles\n",
    "        a = Apossible[t][np.random.randint(len(Apossible[t]))]\n",
    "\n",
    "        # Enregistrer l'action prise\n",
    "        A[t] = a\n",
    "\n",
    "         # Effectuer une action\n",
    "        R[t] = bandit.action(a)\n",
    "\n",
    "        # mise à jour du comptage d'action \n",
    "        N[:,t+1] = N[:,t]\n",
    "        N[a,t+1] += 1\n",
    "\n",
    "        # Mettre à jour les estimations de la valeur d'action, progressivement\n",
    "        if N[a,t] > 0:\n",
    "            Q[:,t+1] = Q[:,t]\n",
    "            Q[a,t+1] = Q[a,t] + (R[t] - Q[a,t]) / N[a,t]\n",
    "        else:\n",
    "            Q[:,t+1] = Q[:,t]\n",
    "            Q[a,t+1] = R[t]\n",
    "\n",
    "    return {'bandit' : bandit,\n",
    "            'numsteps' : netape,\n",
    "            'epsilon' : epsilon,\n",
    "            'Apossible': Apossible, \n",
    "            'A': A, 'N' : N, 'R' : R, 'Q' : Q}\n",
    "\n",
    "k=10   \n",
    "numetape = 300\n",
    "numaction = 2000\n",
    "epsilon = 0.1\n",
    "\n",
    "moyenR = np.zeros((numetape, ))\n",
    "for task in range(2000):\n",
    "    bandit_task = action_greedy(k,numetape, epsilon)\n",
    "    moyenR += bandit_task['R']\n",
    "moyenR /= numaction\n",
    "\n",
    "plt.plot(moyenR) ;\n",
    "plt.ylabel('Reward Moyen') ;\n",
    "plt.xlabel('Etapes') ;\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2-Ronforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Policy evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=1\n",
    "r=-1\n",
    "taille=4\n",
    "k0=np.zeros((4, 4))\n",
    "k=10\n",
    "actions=[[-1,0],[1,0],[0,1],[0,-1]]\n",
    "etat_initial_final=[[0,0], [taille-1, taille-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reaward(position_initaile, action):\n",
    "    \n",
    "    if position_initaile in etat_initial_final:\n",
    "        return position_initaile, 0\n",
    "    \n",
    "    reward = r\n",
    "    position_final = np.array(position_initaile) + np.array(action)\n",
    "    if -1 in position_final or 4 in position_final: \n",
    "        position_final = position_initaile\n",
    "        \n",
    "    return position_final, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "etats = [[i, j] for i in range(taille) for j in range(taille)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "K = 2\n",
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n",
      "\n",
      "K = 3\n",
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n",
      "\n",
      "K = 4\n",
      "[[ 0.      -3.0625  -3.84375 -3.96875]\n",
      " [-3.0625  -3.71875 -3.90625 -3.84375]\n",
      " [-3.84375 -3.90625 -3.71875 -3.0625 ]\n",
      " [-3.96875 -3.84375 -3.0625   0.     ]]\n",
      "\n",
      "K = 5\n",
      "[[ 0.        -3.65625   -4.6953125 -4.90625  ]\n",
      " [-3.65625   -4.484375  -4.78125   -4.6953125]\n",
      " [-4.6953125 -4.78125   -4.484375  -3.65625  ]\n",
      " [-4.90625   -4.6953125 -3.65625    0.       ]]\n",
      "\n",
      "K = 6\n",
      "[[ 0.         -4.20898438 -5.50976562 -5.80078125]\n",
      " [-4.20898438 -5.21875    -5.58984375 -5.50976562]\n",
      " [-5.50976562 -5.58984375 -5.21875    -4.20898438]\n",
      " [-5.80078125 -5.50976562 -4.20898438  0.        ]]\n",
      "\n",
      "K = 7\n",
      "[[ 0.         -4.734375   -6.27734375 -6.65527344]\n",
      " [-4.734375   -5.89941406 -6.36425781 -6.27734375]\n",
      " [-6.27734375 -6.36425781 -5.89941406 -4.734375  ]\n",
      " [-6.65527344 -6.27734375 -4.734375    0.        ]]\n",
      "\n",
      "K = 8\n",
      "[[ 0.         -5.2277832  -7.0078125  -7.46630859]\n",
      " [-5.2277832  -6.54931641 -7.08837891 -7.0078125 ]\n",
      " [-7.0078125  -7.08837891 -6.54931641 -5.2277832 ]\n",
      " [-7.46630859 -7.0078125  -5.2277832   0.        ]]\n",
      "\n",
      "K = 9\n",
      "[[ 0.         -5.69622803 -7.6975708  -8.23706055]\n",
      " [-5.69622803 -7.15808105 -7.77856445 -7.6975708 ]\n",
      " [-7.6975708  -7.77856445 -7.15808105 -5.69622803]\n",
      " [-8.23706055 -7.6975708  -5.69622803  0.        ]]\n",
      "\n",
      "K = 10\n",
      "[[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
      " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
      " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
      " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltas = []\n",
    "for i in range(k):\n",
    "    copie_k0 = np.copy(k0)\n",
    "    etat_delta = []\n",
    "    for etat in etats:\n",
    "        poid_reward = 0\n",
    "        for action in actions:\n",
    "            position_final, reward = Reaward(etat, action)\n",
    "            poid_reward += (1/len(actions))*(reward+(gamma*k0[position_final[0], position_final[1]]))\n",
    "        etat_delta.append(np.abs(copie_k0[etat[0], etat[1]]-poid_reward))\n",
    "        copie_k0[etat[0], etat[1]] = poid_reward\n",
    "    deltas.append(etat_delta)\n",
    "    k0 = copie_k0\n",
    "    if i in [0,1,2,3,4,5,6,7,8,9, k-1]:\n",
    "        print(\"K = {}\".format(i+1))\n",
    "        print(k0)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Policy improuvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "env = GridworldEnv()\n",
    "\n",
    "\n",
    "# Taken from Policy Evaluation !\n",
    "\n",
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "\n",
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI environment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.nS):\n",
    "            # The best action we would take under the current policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = one_step_lookahead(s, V)\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            return policy, V\n",
    "        \n",
    "        \n",
    "        \n",
    "policy, v = policy_improvement(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "# Test the value function\n",
    "expected_v = np.array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1,  0])\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "env = GridworldEnv()\n",
    "\n",
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "policy, v = value_iteration(env)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "# Test the value function\n",
    "expected_v = np.array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1,  0])\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3-Ronforcement Learning (Monte-Carlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from plot_utils import plot_blackjack_values, plot_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env):\n",
    "    \"\"\"\n",
    "    Plays a single episode with a set policy in the environment given. Records the state, action \n",
    "    and reward for each step and returns the all timesteps for the episode.\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        probs = [0.8, 0.2] if state[0] > 18 else [0.2, 0.8]\n",
    "        action = np.random.choice(np.arange(2), p=probs)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(episode, Q,returns_sum, N, gamma=1.0):\n",
    "    \"\"\"\n",
    "    For each time step in the episode we carry out the first visit monte carlo method, checking if this is \n",
    "    the first index of this state. Get the discounted reward and add it to the total reward for that \n",
    "    state/action pair. Increment the times we have seen this state action pair and finally update the Q values\n",
    "    \"\"\"\n",
    "    \n",
    "    for s, a, r in episode:\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode) if x[0] == s)\n",
    "            G = sum([x[2]*(gamma**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "            returns_sum[s][a] += G\n",
    "            N[s][a] += 1.0\n",
    "            Q[s][a] = returns_sum[s][a] / N[s][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_predict(env, num_episodes, gamma=1.0):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the primary method. Plays through several episodes of the environment. \n",
    "    \"\"\"\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        episode = play_episode(env)\n",
    "\n",
    "        update_Q(episode, Q, returns_sum, N)\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = mc_predict(env, 500000)\n",
    "\n",
    "#get the state value function for our test policy\n",
    "V_to_plot = dict((k,(k[0]>18)*(np.dot([0.8, 0.2],v)) + (k[0]<=18)*(np.dot([0.2, 0.8],v))) \\\n",
    "         for k, v in Q.items())\n",
    "\n",
    "# plot the state value functions\n",
    "plot_blackjack_values(V_to_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
